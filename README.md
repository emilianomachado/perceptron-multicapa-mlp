# PerceptrÃ³n Multicapa (MLP)

ImplementaciÃ³n de un **PerceptrÃ³n Multicapa** (Multilayer Perceptron, MLP) utilizando Python en Google Colab.  
Este proyecto muestra cÃ³mo construir y entrenar una red neuronal sencilla desde cero, sin usar frameworks de alto nivel como TensorFlow o PyTorch, para comprender los fundamentos del aprendizaje profundo.

---

## ğŸ§  Â¿QuÃ© es un PerceptrÃ³n Multicapa?

Un **MLP** es una red neuronal feedforward compuesta por:

- Una capa de entrada
- Una o mÃ¡s capas ocultas
- Una capa de salida

Cada neurona aplica una funciÃ³n de activaciÃ³n no lineal (por ejemplo, sigmoide o ReLU), lo que permite al modelo aprender problemas mÃ¡s complejos y **no linealmente separables**, a diferencia del perceptrÃ³n simple.

El entrenamiento se realiza mediante:

- **PropagaciÃ³n hacia adelante (forward propagation)**
- **RetropropagaciÃ³n del error (backpropagation)**  
  ajustando los pesos con gradiente descendente.

---

## ğŸ“‚ Contenido del proyecto

Este repositorio incluye:

- `Perceptron_Multicapa.ipynb`  
  Notebook con:
  - ImplementaciÃ³n completa del MLP
  - DefiniciÃ³n de arquitectura: capas, neuronas y funciones de activaciÃ³n
  - CÃ¡lculo del forward pass
  - ImplementaciÃ³n de backpropagation desde cero
  - Entrenamiento con distintas Ã©pocas y tasas de aprendizaje
  - EvaluaciÃ³n del desempeÃ±o del modelo
  - GrÃ¡ficos y anÃ¡lisis (si corresponden)

---

## ğŸš€ Â¿CÃ³mo ejecutar el proyecto?

1. AbrÃ­ el notebook en Google Colab.
2. EjecutÃ¡ todas las celdas en orden.
3. PodÃ©s modificar:
   - Cantidad de capas ocultas
   - NÃºmero de neuronas por capa
   - FunciÃ³n de activaciÃ³n
   - Tasa de aprendizaje
   - NÃºmero de iteraciones (epochs)
   - Dataset

Para experimentar con distintas arquitecturas y observar cÃ³mo cambia el rendimiento.

---

## ğŸ›  Requisitos

El notebook estÃ¡ pensado para ejecutarse directamente en Google Colab.

LibrerÃ­as utilizadas (todas vienen instaladas por defecto en Colab):

- NumPy  
- Matplotlib (si incluye grÃ¡ficos)

---

## ğŸ“ˆ Resultados

El notebook permite visualizar:

- La evoluciÃ³n del error/funciÃ³n de costo
- Pesos finales aprendidos
- Salidas del modelo
- ComparaciÃ³n entre predicciones y valores reales
- ComprobaciÃ³n de capacidad de generalizaciÃ³n

---

## ğŸ§ª Conceptos clave implementados

- Redes feedforward
- No linealidad mediante funciones de activaciÃ³n
- Regla de actualizaciÃ³n por gradiente descendente
- Backpropagation manual
- Entrenamiento supervisado
- EvaluaciÃ³n sobre nuevos patrones

---

## ğŸ“„ Licencia

Este proyecto es de uso libre para fines educativos y experimentales.

---

## âœ¨ Autor

Proyecto realizado por **Emiliano Machado** como parte del estudio de redes neuronales y aprendizaje profundo.
